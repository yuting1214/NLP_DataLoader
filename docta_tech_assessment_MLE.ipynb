{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Reformat a public dataset.\n",
    "\n",
    "### Objective\n",
    "\n",
    "To enhance the suitability of public datasets for LLM (Large Language Model) training and fine-tuning, datasets need to be presented in a consistent, structured format. Your responsibility is to conceive and implement a data format, then modify a given public dataset to adhere to this new structure.\n",
    "\n",
    "### Detailed Instructions:\n",
    "\n",
    "1. **Dataset Selection**: \n",
    "\n",
    "   - You may start with the [Emotional-Support-Conversation dataset](https://raw.githubusercontent.com/thu-coai/Emotional-Support-Conversation/main/ESConv.json) for this task. But you are encouraged to use other public datasets as long as specific reasons are given.\n",
    "\n",
    "   - This dataset doesn't inherently have \"labels\". Your task includes crafting at least one appropriate label key and annotating the data accordingly.\n",
    "   \n",
    "   - When designing labels, ensure they align with the principles of making the LLM harmless, helpful, and honest.\n",
    "\n",
    "2. **Dataset Attributes**:\n",
    "   - Your reformatted dataset should includes at least two attributes: `raw_data` and `processed_data`.\n",
    "\n",
    "     - `raw_data`: A string, which directly saves the raw text data loaded from the original dataset\n",
    "     \n",
    "     - `processed_data`: A list where each item signifies a feature-label pair that's been processed for a specific task. For clarity, refer to the examples provided in the block below.\n",
    "\n",
    "3. **Flexibility in Design**:\n",
    "\n",
    "   - **Data Structure**: Your design should be accommodating. This means:\n",
    "   \n",
    "     - Simplifying the addition of new processed data.\n",
    "\n",
    "     - Expanding the label classes without hassle (i.e., introducing new label keys).\n",
    "\n",
    "     - Incorporating label values from various annotators (i.e., multiple label values from different annotators for the same label key).\n",
    "     \n",
    "   - **Code Flexibility**: Ensure your code is modular, making it straightforward to apply the same formatting to other public datasets.\n",
    "\n",
    "\n",
    "4. **Design Autonomy**: \n",
    "\n",
    "   - While the example below offers guidance, don't feel restricted by it. If you believe a different structure is more suitable, present your unique design. However, ensure that it incorporates the essential attributes: `raw_data` and `processed_data`.\n",
    "   \n",
    "5. **Deliverables**:\n",
    "\n",
    "   - An outline of your designed data format/structure.\n",
    "\n",
    "   - The code used to convert the public dataset to your design.\n",
    "\n",
    "   - Print the time cost for saving and loading your designed dataset, specifically,\n",
    "\n",
    "     - Time cost for saving the whole dataset\n",
    "\n",
    "     - Time cost for loading the whole dataset\n",
    "     \n",
    "     - Time cost for loading randomly selected 1k instances from the dataset.\n",
    "   \n",
    "\n",
    "   - Model Inference\n",
    "\n",
    "     - Select a Language Model: Choose a language model of your preference. For instance, you can use a pretrained model available from HuggingFace transformers.\n",
    "\n",
    "     - Generate Embeddings:  Utilize the selected model to generate embeddings for your processed data. You only need to obtain embeddings for a sample of 100 instances.\n",
    "\n",
    "     - Find the Closest Pair: Identify the pair of instances with the closest embeddings.\n",
    "\n",
    "     - Display the Instances: Print or display the instances for review.\n",
    "\n",
    "   - **Model Training (Bonus)**\n",
    "\n",
    "      - Select a Language Model: Choose a language model of your preference. For instance, you can use a pretrained model available from HuggingFace transformers.\n",
    "\n",
    "      - Fine-tune the Model: Finetune the pretrained model using your processed data with a training-to-test data split ratio of 7:3. \n",
    "\n",
    "         - You can use a subset of your processed data if the computing resource is limited.\n",
    "         \n",
    "         - The task for fine-tuning could be classification, generation, or embedding improvement. \n",
    "\n",
    "      - Display Training Log: print the training log, which should includes essential information including the training loss and test loss.\n",
    "\n",
    "\n",
    "\n",
    "**Please finish this task in one week. You can return .py or .ipynb files.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
